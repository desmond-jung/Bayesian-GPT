{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b313bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5164b467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "555485f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt\"\n",
    "filename = \"TinyStoriesV2-GPT4-train.txt\"\n",
    "\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "print(\"Download complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3d7eb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 stories.\n",
      "Sample story: Once upon a time, there was a reliable otter named Ollie. He lived in a river with his family. They all loved to play and swim together.\n",
      "One day, Ollie's mom said, \"Ollie, hurry and get some fish for dinner!\" Ollie swam fast to catch fish. He saw his friend, the duck. \"Hi, Ollie!\" said the duck. \"Hi, duck!\" said Ollie. \"I need to hurry and catch fish for my family.\"\n",
      "While Ollie was catching fish, he found a big shiny stone. He thought, \"This is not a fish, but it is so pretty!\" Ollie took the shiny stone home to show his family. They all looked at the shiny stone and smiled. The shiny stone made everyone happy, and they forgot about the fish for dinner.\n"
     ]
    }
   ],
   "source": [
    "with open(\"TinyStoriesV2-GPT4-train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "all_stories = raw_text.split(\"<|endoftext|>\")\n",
    "\n",
    "all_stories = [story.strip() for story in all_stories if story.strip()]\n",
    "\n",
    "stories_subset = all_stories[:100]\n",
    "\n",
    "print(f\"Loaded {len(stories_subset)} stories.\")\n",
    "print(\"Sample story:\", stories_subset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2244cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 100 prompts.\n",
      "Sample prompt: Once upon a time, there was a polite farm dog named Spot.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "prompts = []\n",
    "\n",
    "for story in stories_subset:\n",
    "    match = re.split(r'(?<=[.!?])\\s+', story, maxsplit=1)\n",
    "    if match:\n",
    "        prompt = match[0]\n",
    "        prompts.append(prompt)\n",
    "print(f\"Extracted {len(prompts)} prompts.\")\n",
    "print(\"Sample prompt:\", prompts[99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1ccbe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model, tokenizer, max_new_tokens=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,  # how many *new* tokens to generate\n",
    "        do_sample=True,                 # use sampling instead of greedy decoding\n",
    "        temperature=0.9,                # controls randomness\n",
    "        top_k=50,                       # top-k sampling\n",
    "        top_p=0.95,                     # nucleus sampling\n",
    "        repetition_penalty=1.2,         # penalize repeating phrases\n",
    "        no_repeat_ngram_size=3,         # avoid short n-gram loops\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac99b040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time there was a little boy named Ben. When they saw him, he became more of the same than his father\\'s brother and auntie; but to this day some people have said that it came from an imaginary being who wished never again for any help with their own family.\" - Charles Dickens (13)\\nYou\\'ll be able to see how all kinds go wrong if you look closely at Wikipedia page on The Little Doctor Who story \"The Lord Of Shadow\". A man in Blackadder Street goes through most stories after having died while walking past'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = []\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    full_story = generate_text(prompt, model, tokenizer)\n",
    "    generated_text.append(full_story)\n",
    "\n",
    "generated_text[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aafc2e",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c21e6",
   "metadata": {},
   "source": [
    "Predictive Qualities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6adc458",
   "metadata": {},
   "source": [
    "Perplexity - ability to predict the next word in sequence (1 is perfect), calculated based on the likelihood of the generated tokens given the model's training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1bb4393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings, labels=encodings[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "    return math.exp(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d5639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37.475495546998715"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_perplexity(model, tokenizer, generated_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e599f5",
   "metadata": {},
   "source": [
    "BLEU, ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "acaca3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.06726828559805573\n",
      "ROUGE: {'rouge1': np.float64(0.2518043131539909), 'rouge2': np.float64(0.09347634019934056), 'rougeL': np.float64(0.17336363995553644), 'rougeLsum': np.float64(0.20800055106219872)}\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "bleu = load(\"bleu\")\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# For each generated and reference pair\n",
    "results = bleu.compute(predictions=generated_text, references=stories_subset)\n",
    "print(\"BLEU:\", results[\"bleu\"])\n",
    "\n",
    "results = rouge.compute(predictions=generated_text, references=stories_subset)\n",
    "print(\"ROUGE:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfa865c",
   "metadata": {},
   "source": [
    "Diversity / Creativity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ffa129",
   "metadata": {},
   "source": [
    "Self-BLEU (penalizes similar generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1128a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def compute_self_bleu(texts):\n",
    "    scores = []\n",
    "    for i in range(len(texts)):\n",
    "        references = [t.split() for j, t in enumerate(texts) if j != i]\n",
    "        hypothesis = texts[i].split()\n",
    "        score = sentence_bleu(references, hypothesis)\n",
    "        scores.append(score)\n",
    "    return sum(scores) / len(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb8e9677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.13/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16357457616414095"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_self_bleu(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352ab177",
   "metadata": {},
   "source": [
    "Distinct-n (n-gram diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c728ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_n(texts, n=2):\n",
    "    all_ngrams = set()\n",
    "    total_ngrams = 0\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "        ngram_list = list(ngrams)\n",
    "        all_ngrams.update(ngram_list)\n",
    "        total_ngrams += len(ngram_list)\n",
    "    return len(all_ngrams) / total_ngrams if total_ngrams > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2bfbbb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7854216087987923"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_n(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b40b26c",
   "metadata": {},
   "source": [
    " Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7957a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def calculate_entropy(texts, n=1):\n",
    "    counter = Counter()\n",
    "    total = 0\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        ngrams = list(zip(*[tokens[i:] for i in range(n)]))\n",
    "        counter.update(ngrams)\n",
    "        total += len(ngrams)\n",
    "    probs = [count / total for count in counter.values()]\n",
    "    entropy = -sum(p * math.log(p) for p in probs)\n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea54e3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.590104667252185"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_entropy(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e6fe3",
   "metadata": {},
   "source": [
    "Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c17d3e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.91 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "_ = generate_text(prompt, model, tokenizer)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Inference time: {end - start:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0cd204",
   "metadata": {},
   "source": [
    "## GPT2 with MC Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "17824d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story_mc_dropout(prompt, num_samples=20, max_new_tokens=100):\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = model_inputs[\"input_ids\"]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        all_probs = []\n",
    "\n",
    "        # MC dropout passes\n",
    "        for _ in range(num_samples):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids)\n",
    "                logits = outputs.logits  # shape: (1, seq_len, vocab_size)\n",
    "                next_token_logits = logits[0, -1, :]  # logits for next token\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "        avg_probs = np.mean(all_probs, axis=0)  # posterior predictive\n",
    "        next_token_id = np.random.choice(len(avg_probs), p=avg_probs)\n",
    "        next_token = torch.tensor([[next_token_id]])\n",
    "\n",
    "        # Append to input\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd94cb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a reliable otter named Ollie. She lived with a cousin too nervous to ever be a Terrier. Her Syndrome performed very well her second term in office, multiple had a Box. Another year, and another mermaid par been implemented, and with the times, that had been exaggerated.8 The Terrier Girl had she not, a seemingly distant cousin, was once in MOST TERRIBLE TIMES.\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "At the bus stop hours of 7 a.m. Mall Ave., for about a\n"
     ]
    }
   ],
   "source": [
    "test = prompts[1]\n",
    "print(generate_story_mc_dropout(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ee2f0",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce4912b",
   "metadata": {},
   "source": [
    "Predictive Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a153e",
   "metadata": {},
   "source": [
    "Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e68a7c",
   "metadata": {},
   "source": [
    "Bleu Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d950f54",
   "metadata": {},
   "source": [
    "Diversity / Creativity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af85cd91",
   "metadata": {},
   "source": [
    "Self-Bleu and ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6b8c8",
   "metadata": {},
   "source": [
    "Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a11cb4e",
   "metadata": {},
   "source": [
    "Distinct-n (n-gram diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f289f8",
   "metadata": {},
   "source": [
    "Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029073c7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
